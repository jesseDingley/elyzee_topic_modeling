{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling on grphclus elyzée communities\n",
    "The data concerns Twitter profiles active during the campaign (from November 2016 to May 2017), and their corresponding tweets and retweets, plus the retweet and mention networks related to these profiles. Each community is made up of a set of twitter users. Each community is a part of a level of a community tree. A community tree exists for a given timestamp. \n",
    "\n",
    "Throughout this notebook, we\n",
    " - Retrieve and preprocess the tweets so that they are ready to be fed into the topic modeling algorithm.\n",
    " - Retrieve the communities and extract the tweets from them.\n",
    " - Build topic models for a given level of a timestamp\n",
    " - Build keyword summarization models for a given community\n",
    " - Study topics of a topic model\n",
    " - Study community topic distributions\n",
    " - Display results as wordclouds\n",
    " \n",
    "NOTE: cells that are in comments (# or \"\"\"...\"\"\") are examples, feel free to uncomment then to try out a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "  ## <b>1.</b> <a href=\"#1\">Retrieve necessary data</a>\n",
    "  \n",
    "  \n",
    " ### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>1.1.</b>  <a href=\"#1-1\">Retrieve tweets</a>\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>1.2.</b>  <a href=\"#1-2\">Retrieve tweets for each user</a>\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>1.3.</b>  <a href=\"#1-3\">Read pickle file containing the communities for the elyzée dataset</a>\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>1.4.</b>  <a href=\"#1-4\">Retrieve all users connections</a>\n",
    "\n",
    "\n",
    " \n",
    " ## <b>2.</b> <a href=\"#2\"> Extract tweets from communities</a>\n",
    " \n",
    " ## <b>3.</b> <a href=\"#3\">Build NLP Models</a>\n",
    " \n",
    " ### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>3.1.</b>  <a href=\"#3-1\">LDA Topic Modeling</a>\n",
    "\n",
    " ### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>3.2.</b>  <a href=\"#3-2\">Keyword Summarization</a>\n",
    "\n",
    " ### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>3.3.</b>  <a href=\"#3-3\">Check parameters and choose model</a>\n",
    "\n",
    " ### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>3.4.</b>  <a href=\"#3-4\">Build a model</a>\n",
    " \n",
    " \n",
    " ### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>3.5.</b>  <a href=\"#3-5\">Update a model</a>\n",
    "\n",
    "\n",
    " ## <b>4.</b> <a href=\"#4\">Study purely the topics</a>\n",
    " \n",
    " \n",
    " ### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>4.1.</b>  <a href=\"#4-1\">Show topics</a>\n",
    "  \n",
    " ### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>4.2.</b>  <a href=\"#4-2\">Get unique words for each topic</a>\n",
    "\n",
    " ### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>4.3.</b>  <a href=\"#4-3\">Intertopic distance map</a>\n",
    "\n",
    "\n",
    " \n",
    " ## <b>5.</b> <a href=\"#5\">Study the topic distributions of communities</a>\n",
    " \n",
    " ## <b>6.</b> <a href=\"#6\">Visualization</a>\n",
    "\n",
    "\n",
    " ### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>6.1.</b>  <a href=\"#6-1\">Produce community topic distribution Word Clouds</a>\n",
    "\n",
    "\n",
    " ### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>6.2.</b>  <a href=\"#6-2\">Visualize keyword summarization (word cloud)</a>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# removing stopwords from tweets\n",
    "from gensim.parsing.preprocessing import preprocess_string, STOPWORDS\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "# for comparing dates\n",
    "from datetime import datetime\n",
    "\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim import models\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "# parameter tuning\n",
    "from gensim.models import CoherenceModel\n",
    "from collections import defaultdict \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# word clouds\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Retrieve necessary data\n",
    "<div id='1'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Retrieve tweets\n",
    "<div id='1-1'></div>\n",
    "(See preProcessTweets.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jtweets.json', 'r') as fp:\n",
    "    jtweets = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPreProcessedTweets = [jtweet['txt_pp'] for jtweet in jtweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Retrieve tweets for each user\n",
    "<div id='1-2'></div>\n",
    "(See getAllUsersTweets.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"allUsersTweets.json\",\"r\") as fp:\n",
    "    allUsersTweets = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Read pickle file containing the communities for the elyzée dataset\n",
    "<div id='1-3'></div>\n",
    "It consists of a graph containing the twitter users witht the tweets, and dictionnary where each key is a date and each value is a community tree \n",
    "\n",
    "The community tree is another graph, where communities are organised by hierarchichal level. In the tree, each node is a twitter user (level 0) or a community (level>0).\n",
    "\n",
    "The children of a node (which is thus a community id) are the sub-communities belonging to that node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, sys\n",
    "pickle_file = \"/home/pgay/grphclus_stuff/elyzee_communities.pck\" \n",
    "#\"/home/pgay/grphclus4py/data/elyzee_communities.pck\"\n",
    "#\"/home/pgay/go/src/grphclus/elyzee_communities.pck\"\n",
    "data = pickle.load(open(pickle_file, 'rb')) #/home/pgay/twitter/elyzee/storage.pck','rb'))\n",
    "\n",
    "\n",
    "def aggregate_childrens(community_tree, level):\n",
    "    \"\"\"\n",
    "    create a dictionnary given the community tree and a level\n",
    "    all the keys are the community ids of the level \"level\" \n",
    "    and the values are the twitter users belonging to this community\n",
    "    \"\"\"\n",
    "    communities = {}\n",
    "    if level == 0:\n",
    "        return {}\n",
    "    com_id_this_level = [ n for n in community_tree.nodes if community_tree.nodes[n]['level'] == level ]\n",
    "    for n in com_id_this_level:\n",
    "        communities[n] = get_all_leaves(community_tree, n)\n",
    "    return communities\n",
    "\n",
    "def get_childrens(G, node_id):\n",
    "    \"\"\"\n",
    "    get the children : the nodes which are directly below in the graph\n",
    "    \"\"\"\n",
    "    return [ n for n in G.neighbors(node_id) if G.nodes[n]['level'] < G.nodes[node_id]['level'] ]\n",
    "\n",
    "def get_all_leaves(community_tree, n):\n",
    "    \"\"\"\n",
    "    get the leaves (i.e. the twitter user) which are below a given community id\n",
    "    \"\"\"\n",
    "    leaves = []\n",
    "    childrens = get_childrens(community_tree, n)\n",
    "    if len(childrens) == 0:\n",
    "        leaves.append(n)\n",
    "    for child in get_childrens(community_tree, n):\n",
    "        leaves += get_all_leaves(community_tree, child)\n",
    "    return leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Retrieve all users connections\n",
    "<div id='1-4'></div>\n",
    "(See getAllUsersConnections.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('allUsersConnections.json', 'r') as fp:\n",
    "    allUsersConnections = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract tweets from communities\n",
    "<div id='2'></div>\n",
    "The next set of functions allow us to extract the tweets from communities. As we know, each community represents a set of users. For each user we shall retrieve their tweets in order to construct a set of tweets for a community. \n",
    "\n",
    "As we know, the data is structured as follows: (community $\\subset$ level $\\subset$ community tree / timestamp). We are interested in constructing topic models for a given level. So more precisely, we contruct a set of tweets for a given level. Also, it is possible to retrieve tweets for all users OR all users and their connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLevelsFromTimeStamp(timeStamp):\n",
    "    \"\"\"\n",
    "    get the levels of a community tree given a timestamp\n",
    "    \n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    \n",
    "    :type timeStamp: str\n",
    "    \n",
    "    :return: a set of levels (set of int's)\n",
    "    :rtype: set\n",
    "    \n",
    "    \"\"\"\n",
    "    communityTree = data['communities'][timeStamp]['community_tree']\n",
    "    \n",
    "    return set([communityTree.nodes[n]['level'] for n in communityTree.nodes ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommunitiesFromLevel(timeStamp,level):\n",
    "    \"\"\"\n",
    "    get the communities of a level given the timestamp\n",
    "    \n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    :param level: is the level for the date (ex: 7)\n",
    "    \n",
    "    :type timeStamp: str\n",
    "    :type level: int\n",
    "    \n",
    "    :return: a dictionary where keys are community IDs and values are lists of userIDs \n",
    "    :rtype: dict\n",
    "    \n",
    "    \"\"\"\n",
    "    return aggregate_childrens(data['communities'][timeStamp]['community_tree'], level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserIDsFromCommunity(timeStamp, level, community, communitiesDict):\n",
    "    \"\"\"\n",
    "    get all the user IDs from community 'community' given a timestamp and level\n",
    "    \n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    :param level: is the level for the date (ex: 7)\n",
    "    :param community: is the community id (ex: b'-10000875')\n",
    "    :param communitiesDict: getCommunitiesFromLevel(timeStamp,level)\n",
    "\n",
    "    \n",
    "    :type timeStamp: str\n",
    "    :type level: int\n",
    "    :type community: {bytes,str}\n",
    "    :type communitiesDict: dict\n",
    "    \n",
    "    :return: list of userIDs  \n",
    "    :rtype: list\n",
    "    \n",
    "    \"\"\"\n",
    "    # NO return list(dict.fromkeys([\"\".join([str(int(s)) for s in userID.split() if s.isdigit()]) for userID in  getCommunitiesFromLevel(timeStamp,level)[community]]))  \n",
    "    return list(dict.fromkeys([\"\".join([str(int(s)) for s in userID.split() if s.isdigit()]) for userID in  communitiesDict[community]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllUsersAndUsersConnectionsFromCommunity(timeStamp, level, community, communitiesDict):\n",
    "    \"\"\"\n",
    "    get all user id's and user connections id's from a community given a timestamp, level and community\n",
    "    \n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    :param level: is the level for the date (ex: 7)\n",
    "    :param community: is the community id (ex: b'-10000875')\n",
    "    :param communitiesDict: getCommunitiesFromLevel(timeStamp,level)\n",
    "\n",
    "    \n",
    "    :type timeStamp: str\n",
    "    :type level: int\n",
    "    :type community: {bytes,str}\n",
    "    :type communitiesDict: dict\n",
    "    \n",
    "    :return: list of allIDs\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    \n",
    "    userIDs = getUserIDsFromCommunity(timeStamp,level,community,communitiesDict)\n",
    "    for userID in userIDs:\n",
    "        if userID == ' ': # FIX THIS BUG\n",
    "            userIDs.append(allUsersConnections[userID])\n",
    "    return list(dict.fromkeys(userIDs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweetsFromAllUsersFromCommunity(timeStamp, level, community, dateMargin, communitiesDict):\n",
    "    \"\"\"\n",
    "    get all tweets from all users (not connections) from community given a level and timestamp\n",
    "    \n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    :param level: is the level for the date (ex: 7)\n",
    "    :param community: is the community id (ex: b'-10000875')\n",
    "    :param dateMargin: is the maximal time (in days) between the date of publication of a tweet and the timeStamp (ex: 10) \n",
    "    :param communitiesDict: getCommunitiesFromLevel(timeStamp,level)\n",
    "\n",
    "    :type timeStamp: str\n",
    "    :type level: int\n",
    "    :type community: {bytes,str}\n",
    "    :type dateMargin: int\n",
    "    :type communitiesDict: dict\n",
    "    \n",
    "    :return: - list of lists. Each sublist is preprocessed tweet tokens. ex: [[\"word1\",\"word2\"],[\"ok\",\"help\",\"nice\"],...]\n",
    "             - string of all raw tweets (each tweet is separated by a full stop (\".\")).\n",
    "\n",
    "    :rtype: (list,str)\n",
    "    \"\"\"\n",
    "    \n",
    "    allUserIDs = getUserIDsFromCommunity(timeStamp, level, community, communitiesDict)\n",
    "    ppTweets = []\n",
    "    rawTweets = []\n",
    "    \n",
    "    for userID in allUserIDs:\n",
    "        \n",
    "        # get preprocessed tweets and raw tweets\n",
    "        #userTweets = [(jtweets[n]['txt_pp'],jtweets[n]['txt_ori']) for n in range(len(jtweets[:nbTweets])) if jtweets[n]['source'] == userID and (datetime.strptime(timeStamp,\"%a %b %d %H:%M:%S %z %Y\")-datetime.strptime(jtweets[n]['created_at'],\"%a %b %d %H:%M:%S %z %Y\")).days < dateMargin]\n",
    "        userTweets = [(tweet['txt_pp'],tweet['txt_ori']) for tweet in allUsersTweets[userID] if (datetime.strptime(timeStamp,\"%a %b %d %H:%M:%S %z %Y\")-datetime.strptime(tweet['created_at'],\"%a %b %d %H:%M:%S %z %Y\")).days < dateMargin]\n",
    "        \n",
    "        # separate preprocessed tweets for raw tweets into two separate lists\n",
    "        ppTweets.append([tweet[0] for tweet in userTweets])\n",
    "        rawTweets.append([tweet[1] for tweet in userTweets])\n",
    "        \n",
    "    return [item for sublist in ppTweets for item in sublist], \". \".join(list(dict.fromkeys([item for sublist in rawTweets for item in sublist])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweetsFromAllUsersAndUsersConnectionsFromCommunity(timeStamp, level, community, dateMargin, communitiesDict):\n",
    "    \"\"\"\n",
    "    get all tweets from all users and users connections from community given a timestamp and level\n",
    "    \n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    :param level: is the level for the date (ex: 7)\n",
    "    :param community: is the community id (ex: b'-10000875')\n",
    "    :param dateMargin: is the maximal time (in days) between the date of publication of a tweet and the timeStamp (ex: 10) \n",
    "    :param communitiesDict: getCommunitiesFromLevel(timeStamp,level)\n",
    "\n",
    "    :type timeStamp: str\n",
    "    :type level: int\n",
    "    :type community: {bytes,str}\n",
    "    :type dateMargin: int\n",
    "    :type communitiesDict: dict\n",
    "    \n",
    "    :return: - list of lists. Each sublist is preprocessed tweet tokens. ex: [[\"word1\",\"word2\"],[\"ok\",\"help\",\"nice\"],...]\n",
    "             - string of all raw tweets (each tweet is separated by a full stop (\".\")).\n",
    "\n",
    "    :rtype: (list,str)\n",
    "    \"\"\"\n",
    "    \n",
    "    allUserIDs = getAllUsersAndUsersConnectionsFromCommunity(timeStamp, level, community, communitiesDict)\n",
    "    ppTweets = []\n",
    "    rawTweets = []\n",
    "\n",
    "    for userID in allUserIDs:\n",
    "        \n",
    "        # get preprocessed tweets and raw tweets\n",
    "        #userTweets = [(jtweets[n]['txt_pp'],jtweets[n]['txt_ori']) for n in range(len(jtweets[:nbTweets])) if jtweets[n]['source'] == userID and (datetime.strptime(timeStamp,\"%a %b %d %H:%M:%S %z %Y\")-datetime.strptime(jtweets[n]['created_at'],\"%a %b %d %H:%M:%S %z %Y\")).days < dateMargin]\n",
    "        userTweets = [(tweet['txt_pp'],tweet['txt_ori']) for tweet in allUsersTweets[userID] if (datetime.strptime(timeStamp,\"%a %b %d %H:%M:%S %z %Y\")-datetime.strptime(tweet['created_at'],\"%a %b %d %H:%M:%S %z %Y\")).days < dateMargin]\n",
    "\n",
    "        # separate preprocessed tweets for raw tweets into two separate lists\n",
    "        ppTweets.append([tweet[0] for tweet in userTweets])\n",
    "        rawTweets.append([tweet[1] for tweet in userTweets])\n",
    "        \n",
    "    return [item for sublist in ppTweets for item in sublist], \". \".join(list(dict.fromkeys([item for sublist in rawTweets for item in sublist])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweetsFromAllUsersFromAllCommunitiesFromLevel(timeStamp, level, dateMargin):\n",
    "    \"\"\"\n",
    "    get all tweets from all users from all communities given a level and timestamp\n",
    "    \n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    :param level: is the level for the date (ex: 7)\n",
    "    :param dateMargin: is the maximal time (in days) between the date of publication of a tweet and the timeStamp (ex: 10) \n",
    "    \n",
    "    :type timeStamp: str\n",
    "    :type level: int\n",
    "    :type dateMargin: int\n",
    "    \n",
    "    :return: dictionary where keys are communities and values are list of tweets\n",
    "             example: {b'-100000765': ([['word','ok'],['hello','good'],...],\" raw tweets\"),\n",
    "                       b'-100006795': ([['word','ok'],['hello','good'],...],\" raw tweets\"),\n",
    "                       ...}\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    \n",
    "    comsTweets = {}\n",
    "    communitiesDict = getCommunitiesFromLevel(timeStamp,level)\n",
    "    \n",
    "    for community in communitiesDict:\n",
    "        comsTweets[community] = getTweetsFromAllUsersFromCommunity(timeStamp, level, community, dateMargin, communitiesDict) \n",
    "        \n",
    "    return comsTweets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweetsFromAllUsersAndUsersConnectionsFromAllCommunitiesFromLevel(timeStamp, level, dateMargin):\n",
    "    \"\"\"\n",
    "    get all tweets from all users (and their connections) from all communities given a level and timestamp\n",
    "    \n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    :param level: is the level for the date (ex: 7)\n",
    "    :param dateMargin: is the maximal time (in days) between the date of publication of a tweet and the timeStamp (ex: 10) \n",
    "    \n",
    "    :type timeStamp: str\n",
    "    :type level: int\n",
    "    :type dateMargin: int\n",
    "    \n",
    "    :return: dictionary where keys are communities and values are list of tweets\n",
    "             example: {b'-100000765': ([['word','ok'],['hello','good'],...],\" raw tweets\"),\n",
    "                       b'-100006795': ([['word','ok'],['hello','good'],...],\" raw tweets\"),\n",
    "                       ...}\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    \n",
    "    comsTweets = {}\n",
    "    communitiesDict = getCommunitiesFromLevel(timeStamp,level)\n",
    "\n",
    "    for community in communitiesDict:\n",
    "        comsTweets[community] = getTweetsFromAllUsersAndUsersConnectionsFromCommunity(timeStamp, level, community, dateMargin, communitiesDict) \n",
    "    \n",
    "    return comsTweets    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build NLP models\n",
    "<div id='3'></div>\n",
    "Up to now, we have done all the preparation needed in order to be able to build some NLP models, more precisely text summarization models. In this notebook, we're primarily focusing on Topic Modeling. \n",
    "\n",
    "To give a brief recap, <b>Topic Modeling</b> consists of building a set of topics to describe a set of documents. In `LDAModelForLevel()`, we consider a document as a community and the set of documents as a level. This is for creayting topic models on the level of a level. In `LDAModelForAllTweets()`, we consider a document as a tweet and the set of documents as all the tweets in the dataset. This is for creating a general topic model covering all aspects of the clustering. We'll focus more on this function.  Each topic is composed of a set of keywords. It is up to the user to decide what the topic is about from the keywords. Then, we assign each community a topic distribution. For example, community x could be made up of topic 1 at 80% and topic 5 at 20%. So topic 1 is the most prevalent topic in this community.\n",
    "\n",
    "The other model presented in this notebook is <b>Keyword Summarization</b> which consists in extracting the most prevalent words in a document. So Text Summarization isn't necessarily interesting as Topic Modeling as it only gives a general view of a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. LDA Topic Modeling\n",
    "<div id='3-1'></div>\n",
    "To build our topic model, we will use the LDA (Latent Dirichlet Allocation) approach. LDA is a form of unsupervised learning that views documents as bags of words meaning order doesn't matter. We're not going to go into the details here, see https://towardsdatascience.com/lda-topic-modeling-an-explanation-e184c90aadcd for more details.\n",
    "\n",
    "In our case, we will procede with the <b>gensim</b> library (https://radimrehurek.com/gensim/) to construct our LDA topic model. gensim comes with a built in function for building LDA models, called `LdaMultiCore`. 'multicore' to speed up processing. This function allows for a lot of hyper-parameter tuning. The important ones are as following: \n",
    " - <b> num_topics </b> (int) (default = 100): number of topics of the model.\n",
    " - <b>passes</b> (int) (default = 1): controls how often we train the model on the entire corpus (set to 10)\n",
    " - <b>iterations</b> (int) (default = 50). This parameter is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. iterations is the aximum number of iterations through the corpus when inferring the topic distribution of a corpus. It is important to set the number of “passes” and “iterations” high enough.\n",
    " - <b>alpha</b> ({float,str}) (default='symmetric'): Document-Topic Density. with a higher alpha, documents are assumed to be made up of more topics and result in more specific topic distribution per document.\n",
    " - <b>eta</b> ({float,str}) (default=None): Topic-Word Density. with high beta, topics are assumed to made of up most of the words and result in a more specific word distribution per topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDAModelForLevel(timeStamp, level, dateMargin, connectionsToo, numTopics, passes, ppComsDict, rawComsDict, alpha, eta, eval_every, iterations):\n",
    "    \"\"\"\n",
    "    construct LDA topic model for a level given a timestamp\n",
    "    \n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    :param level: is the level for the date (ex: 7)\n",
    "    :param dateMargin: is the maximal time (in days) between the date of publication of a tweet and the timeStamp (ex: 10) \n",
    "    :param connectionsToo: true if you want users connections tweets too, false if not\n",
    "    :param numTopics: number of topic for topic model\n",
    "    :param passes: controls how often we train the lda model on the entire corpus (set to 1)\n",
    "    :param ppComsDict: preprocessed coms as a dict. For running tests on existing models\n",
    "    :param rawComsDict: raw coms as a dict. For running tests on existing models\n",
    "    :param alpha: Document-Topic Density.\n",
    "    :param eta: Topic-Wird Density.\n",
    "    :param eval_every: log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x (set to 10)\n",
    "    :param iterations: Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
    "\n",
    "    :type timeStamp: str\n",
    "    :type level: int\n",
    "    :type dateMargin: int\n",
    "    :type connectionsToo: bool\n",
    "    :type numTopics: int\n",
    "    :type passes: int\n",
    "    :type ppComsDict: {dict,NoneType}\n",
    "    :type rawComsDict: {dict,NoneType}\n",
    "    :type alpha: {float,str}\n",
    "    :type eta: {float,str}\n",
    "    :type eval_every: int\n",
    "    :type iterations: int\n",
    "    \n",
    "    :return: LDA Model, dictionary, corpus, corpusTransformedSorted, ppComsDict, rawComsDict\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # check if we have already collected the tweet data. If not then we need to do it again.\n",
    "    # This process of getting the tweets can take a while so for testing it's good if we don't have to do it twice.\n",
    "    if (ppComsDict == None) and (rawComsDict == None):\n",
    "        \n",
    "        # get tweets from level\n",
    "        if connectionsToo:\n",
    "            coms = getTweetsFromAllUsersAndUsersConnectionsFromAllCommunitiesFromLevel(timeStamp, level, dateMargin)\n",
    "        else:\n",
    "            coms = getTweetsFromAllUsersFromAllCommunitiesFromLevel(timeStamp, level, dateMargin)\n",
    "\n",
    "        # separate the raw tweets from the pre processed ones\n",
    "        rawComsDict = {k:v[1] for (k,v) in coms.items()}\n",
    "        ppComsDict = {k:v[0] for (k,v) in coms.items()}\n",
    "\n",
    "    # concatenate all the tweets of a community into one list.\n",
    "    # So we get a list where each elt is a list of words. ex [[\"word\",\"hello\",...],[\"well\",\"ok\",...],...]\n",
    "    #                                                              com1                com2\n",
    "    preProcessedComs = [[item for sublist in tweets for item in sublist] for tweets in list(ppComsDict.values())]\n",
    "\n",
    "    dictionaryComs = corpora.Dictionary(preProcessedComs)\n",
    "    corpusComs = [dictionaryComs.doc2bow(community) for community in preProcessedComs]\n",
    "\n",
    "    # create model\n",
    "    model = models.ldamulticore.LdaMulticore(corpus=corpusComs, \n",
    "                                     num_topics=numTopics, \n",
    "                                     id2word=dictionaryComs, \n",
    "                                     passes=passes,\n",
    "                                     alpha=alpha,\n",
    "                                     eta=eta,\n",
    "                                     eval_every=eval_every, \n",
    "                                     iterations=iterations)\n",
    "\n",
    "    # calculate the document-topic distributions (will need adjusting later)\n",
    "    docTopicDist = model[corpusComs]\n",
    "    \n",
    "    # sort each doc-topic distribution by most prevalent topics first \n",
    "    docTopicDistSorted = [sorted(c,key= lambda a: a[1], reverse=True) for c in docTopicDist]\n",
    "    \n",
    "    \"\"\"\n",
    "    !IMPORTANT!\n",
    "    Un-comment the next section of code if you want to save the model as a file.\n",
    "    \"\"\"\n",
    "    #modelName = re.sub(\"[ :]\",\"\",str(timeStamp))+\";\"+str(level)+\".gensim\"\n",
    "    #model.save(modelName)\n",
    "    \n",
    "    #corpusName = re.sub(\"[ :]\",\"\",str(timeStamp))+\";\"+str(level)+\".pkl\"\n",
    "    #pickle.dump(corpusComs, open(corpusName, 'wb'))\n",
    "    \n",
    "    return model, dictionaryComs, corpusComs, docTopicDistSorted, ppComsDict, rawComsDict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDAModelForAllTweets(numTopics, passes, alpha, eta, eval_every, iterations):\n",
    "    \"\"\"\n",
    "    Construct LDA model where in the document-term matrix we have: document = tweet.\n",
    "    We consider all tweets in the dataset\n",
    "    \n",
    "    :param numTopics: number of topic for topic model\n",
    "    :param passes: controls how often we train the lda model on the entire corpus (set to 1)\n",
    "    :param alpha: Document-Topic Density.\n",
    "    :param eta: Topic-Wird Density.\n",
    "    :param eval_every: log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x (set to 10)\n",
    "    :param iterations: Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
    "\n",
    "    :type numTopics: int\n",
    "    :type passes: int\n",
    "    :type alpha: {float,str}\n",
    "    :type eta: {float,str}\n",
    "    :type eval_every: int\n",
    "    :type iterations: int\n",
    "    \n",
    "    :return: model, dictionary, corpus, allPreProcessedTweets\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(allPreProcessedTweets)\n",
    "    corpus = [dictionary.doc2bow(tweet) for tweet in allPreProcessedTweets]\n",
    "    \n",
    "    # create model\n",
    "    model = models.ldamulticore.LdaMulticore(\n",
    "                                     corpus=corpus, \n",
    "                                     num_topics=numTopics, \n",
    "                                     id2word=dictionary, \n",
    "                                     passes=passes,\n",
    "                                     alpha=alpha,\n",
    "                                     eta=eta,\n",
    "                                     eval_every=eval_every, \n",
    "                                     iterations=iterations)\n",
    "    \n",
    "    \"\"\"\n",
    "    !IMPORTANT!\n",
    "    Un-comment the next section of code if you want to save the model as a file.\n",
    "    \"\"\"\n",
    "    #modelName = re.sub(\"[ :]\",\"\",str(timeStamp))+\";\"+str(level)+\".gensim\"\n",
    "    #model.save(modelName)\n",
    "    \n",
    "    #corpusName = re.sub(\"[ :]\",\"\",str(timeStamp))+\";\"+str(level)+\".pkl\"\n",
    "    #pickle.dump(corpus, open(corpusName, 'wb'))\n",
    "    \n",
    "    return model, dictionary, corpus, allPreProcessedTweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Keyword Summarization\n",
    "<div id='3-2'></div>\n",
    "First and foremost, this summarization only applies to a specific community, not a level (for now). For the keyword summarization we will use <b>gensim</b> again. gensim has a keyword summarization function `keywords`. This function isn't great so we do a little bet extra processing. We filter out irrelevent words and limit the number of keywords to describe a community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInitialKeyWords(timeStamp, level, community, dateMargin, connectionsToo):\n",
    "    \"\"\"\n",
    "    perform a first key word summarization (nothing fancy)\n",
    "    \n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    :param level: is the level for the date (ex: 7)\n",
    "    :param community: community id\n",
    "    :param dateMargin: is the maximal time (in days) between the date of publication of a tweet and the timeStamp (ex: 10) \n",
    "    :param connectionsToo: true if you want users connections tweets too, false if not\n",
    "    \n",
    "    :type timeStamp: str\n",
    "    :type level: int\n",
    "    :type community: {bytes,str}\n",
    "    :type dateMargin: int\n",
    "    :type connectionsToo: bool\n",
    "    \n",
    "    returns lists of keywords for the associated com \n",
    "    \"\"\"\n",
    "    \n",
    "    communitiesDict = getCommunitiesFromLevel(timeStamp,level)\n",
    "    if connectionsToo:\n",
    "        allUsersIDs = getAllUsersAndUsersConnectionsFromCommunity(timeStamp, level, community, communitiesDict)\n",
    "    else:\n",
    "        allUsersIDs = getUserIDsFromCommunity(timeStamp, level, community, communitiesDict)\n",
    "        \n",
    "    tweets = []\n",
    "    for userID in allUsersIDs:\n",
    "        \n",
    "        # get semi-preprocssed tweet data (it's good enough for now)\n",
    "        rawUserTweets = [tweet['txt']for tweet in allUsersTweets[userID] if tweet['source'] == userID and (datetime.strptime(timeStamp,\"%a %b %d %H:%M:%S %z %Y\")-datetime.strptime(tweet['created_at'],\"%a %b %d %H:%M:%S %z %Y\")).days < dateMargin]\n",
    "        tweets.append(rawUserTweets)\n",
    "        \n",
    "    tweets = list(set([item for sublist in tweets for item in sublist]))\n",
    "    \n",
    "    return keywords('. '.join(tweets), scores=True, lemmatize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplifyKeyWords(nbKeyWords,minWeight,communityKeyWordsU):\n",
    "    \"\"\"\n",
    "    reduce number of keywords, filter out irrelevent words and sort\n",
    "    \n",
    "    :param nbKeyWords: number of keywords to describe community desired\n",
    "    :param minWeight: minimal weight for keywords\n",
    "    :param communityKeyWordsU: list of initial keywords (from getInitialKeyWords())\n",
    "    \n",
    "    :type nbKeyWords: int\n",
    "    :type minWeight: float\n",
    "    :type communityKeyWordsU: list\n",
    "    \n",
    "    :return: list of keywords\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    return  sorted([keyword for keyword in communityKeyWordsU if keyword[1] > minWeight and keyword[0].count(' ') <= 2],key=lambda x:x[1],reverse=True)[:nbKeyWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KeyWordModelForCommunity(timeStamp, level, community, dateMargin, connectionsToo, nbKeyWords, minWeight):\n",
    "    \"\"\"\n",
    "    summing up a community of a level by the keywords extracted from the concatenation of its tweets\n",
    "    \n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    :param level: is the level for the date (ex: 7)\n",
    "    :param community: community id\n",
    "    :param dateMargin: is the maximal time (in days) between the date of publication of a tweet and the timeStamp (ex: 10) \n",
    "    :param connectionsToo: true if you want users connections tweets too, false if not\n",
    "    :param nbKeyWords: number of keywords to describe community desired\n",
    "    :param minWeight: minimal weight for keywords\n",
    "    \n",
    "    \n",
    "    :type timeStamp: str\n",
    "    :type level: int\n",
    "    :type community: {bytes,str}\n",
    "    :type dateMargin: int\n",
    "    :type connectionsToo: bool\n",
    "    :type nbKeyWords: int\n",
    "    :type minWeight: float\n",
    "    \n",
    "    :return: list of keywords\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    return simplifyKeyWords(nbKeyWords, minWeight, getInitialKeyWords(timeStamp, level, community, dateMargin, connectionsToo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Check parameters and choose model\n",
    "<div id='3-3'></div>\n",
    "In the previous section of this notebook, we developed some functions to make Topic Models and perform Keyword Summarization. The following function checks that all the parameters are valid and allows us to choose between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildModelForLevel(timeStamp, \n",
    "                       level, \n",
    "                       dateMargin, \n",
    "                       modelType, \n",
    "                       connectionsToo, \n",
    "                       nbKeyWords=30, \n",
    "                       minWeight=0.05, \n",
    "                       numTopics=10, \n",
    "                       community=None, \n",
    "                       passes=1, \n",
    "                       ppComsDict=None,\n",
    "                       rawComsDict=None,\n",
    "                       alpha='symmetric',\n",
    "                       eta=None,\n",
    "                       eval_every=10,\n",
    "                       iterations=50):\n",
    "    \"\"\"\n",
    "    construct NLP Model for a level given a timestamp.\n",
    "    \n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    :param level: is the level for the date (ex: 7)\n",
    "    :param dateMargin: is the maximal time (in days) between the date of publication of a tweet and the timeStamp (ex: 10) \n",
    "    :param modelType: is the desired model type (0 for topic model, 1 for key word summarization)\n",
    "    :param connectionsToo: true if you want users connections tweets too, false if not\n",
    "    :param nbKeyWords: (optional) number of desired keywords for Key Word Summarization\n",
    "    :param minWeight: (optional) minimal weight of key word for key word summarization\n",
    "    :param numTopics: (optional) number of topic for topic model\n",
    "    :param community: (optional) desired community for key word summarization\n",
    "    :param passes: (optional) controls how often we train the lda model on the entire corpus (set to 1)\n",
    "    :param ppComsDict: (optional) preprocessed coms as a dict. For running tests on existing models\n",
    "    :param rawComsDict: (optional) raw coms as a dict. For running tests on existing models\n",
    "    :param alpha: (optional) Document-Topic Density.\n",
    "    :param eta: (optional) Topic-Word Density.\n",
    "    :param eval_every: (optional) log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x (set to 10)\n",
    "    :param iterations: (optional) Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
    "\n",
    "    :type timeStamp: str\n",
    "    :type level: int\n",
    "    :type dateMargin: int\n",
    "    :type modelType: int\n",
    "    :type connectionsToo: bool\n",
    "    :type nbKeyWords: int\n",
    "    :type minWeight: float\n",
    "    :type numTopics: int\n",
    "    :type community: {bytes,str}\n",
    "    :type passes: int\n",
    "    :type ppComsDict: {dict,NoneType}\n",
    "    :type rawComsDict: {dict,NoneType}\n",
    "    :type alpha: {float,str}\n",
    "    :type eta: {float,str}\n",
    "    :type eval_every: int\n",
    "    :type iterations: int\n",
    "    \n",
    "    :return: model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # check all parameters   \n",
    "    \n",
    "    if not timeStamp in list(data['communities'].keys()):\n",
    "        raise Exception('time stamp non existant')\n",
    "        \n",
    "    if not level in getLevelsFromTimeStamp(timeStamp):\n",
    "        raise Exception(\"Level non existant\")\n",
    "        \n",
    "    if dateMargin < 0:\n",
    "        raise Exception('margin invalid')\n",
    "\n",
    "    if not modelType in [0,1]:\n",
    "        raise Exception(\"model type not valid ,choose from 0 or 1\")\n",
    "        \n",
    "    if not connectionsToo in [True, False]:\n",
    "        raise Exception(\"boolean value for connectionsToo\")\n",
    "        \n",
    "    if nbKeyWords < 1:\n",
    "        raise Exception('not enough key words')\n",
    "        \n",
    "    if minWeight < 0 or minWeight > 1:\n",
    "        raise Exception('min weight needs to be between 0 and 1')\n",
    "        \n",
    "    if numTopics <= 1:\n",
    "         raise Exception('nb of topics has to be at least 1')\n",
    "            \n",
    "    if community != None: \n",
    "        if not community in getCommunitiesFromLevel(timeStamp,level):\n",
    "            raise Exception(\"community non existant\")\n",
    "            \n",
    "    if passes  < 1:\n",
    "        raise Exception(\"passes needs to be at least 1\")\n",
    "        \n",
    "    if type(alpha) == float:\n",
    "        if alpha < 0 or alpha > 1:\n",
    "            raise Exception('alpha needs to between 0 and 1 in the case that it\"s a float')\n",
    "            \n",
    "    if type(alpha) == str:\n",
    "        if not alpha in [\"auto\",\"symmetric\",\"asymmetric\"]:\n",
    "            raise Exception('alpha needs to in [\"auto\",\"symmetric\",\"asymmetric\"] in the case that it\"s a string')\n",
    "            \n",
    "    if eta != None:\n",
    "        if type(eta) == float:\n",
    "            if eta < 0 or eta > 1:\n",
    "                raise Exception('eta needs to between 0 and 1 in the case that it\"s a float')\n",
    "        if type(eta) == str:\n",
    "            if eta != 'symmetric':\n",
    "                raise Exception('eta needs to be equal to \"symmetric\" in the case that it\"s a string')\n",
    "                \n",
    "    if eval_every < 0:\n",
    "        raise Exception(\"eval_every needs to be at least one\")\n",
    "        \n",
    "    if iterations < 50:\n",
    "        raise Exception(\"iterations needs to be at least 50\")\n",
    "\n",
    "    # returns model depending on decision by user\n",
    "    if modelType == 0:\n",
    "        # build topic model\n",
    "        return LDAModelForLevel(timeStamp, level, dateMargin, connectionsToo, numTopics, passes, ppComsDict, rawComsDict, alpha, eta, eval_every, iterations)\n",
    "    else:\n",
    "        # build keyword model\n",
    "        return KeyWordModelForCommunity(timeStamp, level, community, dateMargin, connectionsToo, nbKeyWords, minWeight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Build a model\n",
    "<div id='3-4'></div>\n",
    "Let's build an example topic model. Using the results from our tests we can build a topic model for level 3 of the community tree at Sun Apr 30 10:30:11 +0000 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmyModel, myDictionary, myCorpus, myDocTopicDistSorted, myppComs, myrawComs = BuildModelForLevel(\\n                          timeStamp='Sun Apr 30 10:30:11 +0000 2017',\\n                          level=3,\\n                          dateMargin=500,\\n                          modelType=0,\\n                          connectionsToo=True,\\n                          numTopics=15,\\n                          passes=10,\\n                          eval_every=10,\\n                          iterations=200,\\n                          alpha=0.31,\\n                          eta=0.61)\\n                          #ppComsDict=myppComs,\\n                          #rawComsDict=myrawComs)\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "myModel, myDictionary, myCorpus, myDocTopicDistSorted, myppComs, myrawComs = BuildModelForLevel(\n",
    "                          timeStamp='Sun Apr 30 10:30:11 +0000 2017',\n",
    "                          level=3,\n",
    "                          dateMargin=500,\n",
    "                          modelType=0,\n",
    "                          connectionsToo=True,\n",
    "                          numTopics=15,\n",
    "                          passes=10,\n",
    "                          eval_every=10,\n",
    "                          iterations=200,\n",
    "                          alpha=0.31,\n",
    "                          eta=0.61)\n",
    "                          #ppComsDict=myppComs,\n",
    "                          #rawComsDict=myrawComs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a keyword summarization model too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmyKWModel = BuildModelForLevel(\\n    dateMargin=50,\\n    modelType=1,\\n    timeStamp='Sun Apr 30 10:30:11 +0000 2017',\\n    level=3,\\n    community= b'-30000094722',\\n    connectionsToo=False,\\n    nbKeyWords=50,\\n    minWeight=0.05)\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "myKWModel = BuildModelForLevel(\n",
    "    dateMargin=50,\n",
    "    modelType=1,\n",
    "    timeStamp='Sun Apr 30 10:30:11 +0000 2017',\n",
    "    level=3,\n",
    "    community= b'-30000094722',\n",
    "    connectionsToo=False,\n",
    "    nbKeyWords=50,\n",
    "    minWeight=0.05)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Update a model\n",
    "<div id='3-5'></div>\n",
    "The following function allows us to update a pre existing model with a new community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateLDAModelWithNewCom(model, dictionary, ppComs, rawComs, timeStamp, level, community, connectionsToo, dateMargin):\n",
    "    \n",
    "    \"\"\" \n",
    "    update an LDA model with a new community\n",
    "  \n",
    "    :param model: (gensim.models.ldamulticore): is the old model\n",
    "    :param dictionary: (gensim.corpora): is the old dictionary\n",
    "    :param ppComsDict: preprocessed coms as a dict. For running tests on existing models\n",
    "    :param rawComsDict: raw coms as a dict. For running tests on existing models\n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    :param level: is the level for the date (ex: 7)\n",
    "    :param community: new community id\n",
    "    :param connectionsToo: true if you want users connections tweets too, false if not\n",
    "    :param dateMargin: is the maximal time (in days) between the date of publication of a tweet and the timeStamp (ex: 10) \n",
    "\n",
    "    :type model: gensim.models.ldamulticore\n",
    "    :type dictionary: gensim.corpora\n",
    "    :type ppComsDict: {dict,NoneType}\n",
    "    :type rawComsDict: {dict,NoneType}\n",
    "    :type timeStamp: str\n",
    "    :type level: int\n",
    "    :type community: {bytes,str}    \n",
    "    :type connectionsToo: bool\n",
    "    :type dateMargin: int\n",
    "\n",
    "    :return: model, dictionary, newDocTopicDistSorted, ppComs, rawComs\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get tweets from new com\n",
    "    if connectionsToo:\n",
    "        comTweets = getTweetsFromAllUsersAndUsersConnectionsFromCommunity(timeStamp,level,community, dateMargin)\n",
    "    else:\n",
    "        comTweets = getTweetsFromAllUsersFromCommunity(timeStamp, level, community, dateMargin)\n",
    "    \n",
    "    # add new com to the rest of other coms\n",
    "    ppComs[community] = comTweets[0]\n",
    "    rawComs[community] = comTweets[1]\n",
    "    \n",
    "    preProcessedComs = [[item for sublist in tweets for item in sublist] for tweets in ppComs.values()] \n",
    "    \n",
    "    # create dict\n",
    "    preProcessedCom = [[item for sublist in comTweets[0] for item in sublist]] \n",
    "    comDict = corpora.Dictionary(preProcessedCom)\n",
    "    comCorpus = [comDict.doc2bow(community) for community in preProcessedCom]\n",
    "    \n",
    "    #dictionary.add_documents(preProcessedCom)\n",
    "    newCorpus = [dictionary.doc2bow(community) for community in preProcessedComs]\n",
    "    \n",
    "    model.update(newCorpus)\n",
    "    newDocTopicDist = model[newCorpus]\n",
    "    newDocTopicDistSorted = [sorted(c,key= lambda a: a[1], reverse=True) for c in newDocTopicDist]\n",
    "    \n",
    "    return model, dictionary, newDocTopicDistSorted, ppComs, rawComs # remove ppComs, rawComs maybe\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Study purely the topics\n",
    "<div id='4'></div>\n",
    "As we know, our topic model has a set of topics. This next part of the notebook is purely for just for studying the topics as they are, not topic distributions over communtities. Three possibilites for this:\n",
    " - Show the topics, how many you want and how many words per topic to show.\n",
    " - Get the unique words from each topic (the words that are present in only one topic)\n",
    " - Show the intertopic distance map (see below for further details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Show topics\n",
    "<div id='4-1'></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopics(model,nbTopics,nbWords):\n",
    "    \"\"\"\n",
    "    get topics for LDA Model\n",
    "    \n",
    "    :param model: the model\n",
    "    :param nbTopics: is the number of topics to be displayed\n",
    "    :param nbWords: is the number of words in topics \n",
    "    \n",
    "    :type model: gensim.models.ldamulticore\n",
    "    :type nbTopics: int\n",
    "    :type nbWords: int\n",
    "    \n",
    "    :return: list of topics in the format word1*\"weight1\"+word2*\"weight2\"+...\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    return list(dict.fromkeys([topic for topic in model.print_topics(num_topics=nbTopics, num_words=nbWords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#getTopics(myModel,30,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Get unique words for each topic\n",
    "<div id='4-2'></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopicKeyWords(model,topicNb,nbTopics,nbWords):\n",
    "    \"\"\"\n",
    "    get just the keywords from a topic.\n",
    "    \n",
    "    :param model: the model\n",
    "    :param topicNb: the topic from which we want the keywords\n",
    "    :param nbTopics: is the number of topics to be displayed (see prev func)\n",
    "    :param nbWords: is the number of words in topics \n",
    "    \n",
    "    :type model: gensim.models.ldamulticore\n",
    "    :type topicNb: int\n",
    "    :type nbTopics: int\n",
    "    :type nbWords: int\n",
    "    \n",
    "    \n",
    "    :return: a list of keywords for topic 'topicNb'\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    return [re.sub('\"',\"\",elt.split(\"*\")[1]).replace(\" \",\"\") for elt in [topic for topic in getTopics(model,nbTopics,nbWords) if topic[0] == topicNb][0][1].split(\"+\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopicKeyWordsForAllTopics(model,nbTopics,nbWords):\n",
    "    \"\"\"\n",
    "    return a list of lists. each sublist is a topic's keywords\n",
    "    \n",
    "    :param model: the model\n",
    "    :param nbTopics: is the number of topics to be displayed (see prev func)\n",
    "    :param nbWords: is the number of words in topics \n",
    "    \n",
    "    :type model: gensim.models.ldamulticore\n",
    "    :type nbTopics: int\n",
    "    :type nbWords: int\n",
    "    \n",
    "    :return: list of topic keyword lists\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    return [getTopicKeyWords(model,i,nbTopics,nbWords) for i in range(len(getTopics(model,nbTopics,nbWords)))]\n",
    "    #return [i for i in range(len(getTopics(model,nbTopics,nbWords)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueWordsForeachTopic(model,nbTopics,nbWords):\n",
    "    \"\"\"\n",
    "    return dictionary where keys are topic nbs and values are lists of unique words to the topic\n",
    "    \n",
    "    :param model: the model\n",
    "    :param nbTopics: is the number of topics to be displayed (see prev func)\n",
    "    :param nbWords: is the number of words in topics \n",
    "    \n",
    "    :type model: gensim.models.ldamulticore\n",
    "    :type nbTopics: int\n",
    "    :type nbWords: int\n",
    "    \n",
    "    :return: dictionary where keys are topic nbs and values are lists of unique words to the topic\n",
    "    :rtype: dict\n",
    "    \n",
    "    \"\"\"\n",
    "    reducedTopics = {}\n",
    "    i = 0\n",
    "    for topicWords in getTopicKeyWordsForAllTopics(model,nbTopics,nbWords):\n",
    "        for otherTopicWords in [words for words in getTopicKeyWordsForAllTopics(model,nbTopics,nbWords) if words != topicWords]:\n",
    "            for word in otherTopicWords:\n",
    "                if word in topicWords:\n",
    "                    topicWords.remove(word)\n",
    "        reducedTopics[i] = topicWords\n",
    "        i += 1\n",
    "    return reducedTopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getUniqueWordsForeachTopic(myModel,30,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Intertopic distance map\n",
    "<div id='4-3'></div>\n",
    "The intertopic distance map allows us to display the model topics in a bubble graph. Each bubble corresponds to a topic. The bigger the bubble, the more prevalent that topic is in the corpus. A good topic model will have farely large bubbles not clustered in one place. Overlapping bubbles means that the topics are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intertopicDistanceMap(model, corpus, dictionary):\n",
    "    \"\"\"\n",
    "    intertopic Distance Map (bubbles graph) for LDA model \n",
    "    \"\"\"\n",
    "    \n",
    "    modelVis = pyLDAvis.gensim.prepare(model, corpus, dictionary)\n",
    "    pyLDAvis.enable_notebook()\n",
    "    \n",
    "    return pyLDAvis.gensim.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#intertopicDistanceMap(myModel,myCorpus,myDictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in this case that are model isn't great (30 topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Study the topic distributions of communities\n",
    "<div id='5'></div>\n",
    "Mentioned previously, each community can be represented as a distribution of topics. This next part of the notebook allows us to display these distributions.\n",
    "\n",
    "We can show the distributions for all the communities of the level at once, or see only the topic distribution of one community. \n",
    "\n",
    "For the one community case, it can can be internal to the level or external, meaning being able to study the topic distribution of a community that isn't apart of the original LDA topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getComTopicDist(community, model, minWeight, timeStamp, level, dateMargin, connectionsToo):\n",
    "    \"\"\"\n",
    "    get community topic distribution\n",
    "    \n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    :param level: is the level for the date (ex: 7)\n",
    "    :param community: new community id\n",
    "    :param connectionsToo: true if you want users connections tweets too, false if not\n",
    "    :param model: (gensim.models.ldamulticore): is the old model\n",
    "    :param dateMargin: is the maximal time (in days) between the date of publication of a tweet and the timeStamp (ex: 10) \n",
    "    :param minWeight: minimal weight for a topic in community\n",
    "\n",
    "\n",
    "    :type model: gensim.models.ldamulticore\n",
    "    :type timeStamp: str\n",
    "    :type level: int\n",
    "    :type community: {bytes,str}    \n",
    "    :type connectionsToo: bool\n",
    "    :type dateMargin: int\n",
    "    :type minWeight: float\n",
    " \n",
    "    :return: dict where key is community and value is list of tuples (ex: [(1, 0.8), (2, 0.2)])\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    \n",
    "    levelComs = getCommunitiesFromLevel(timeStamp,level).keys()\n",
    "    \n",
    "    if connectionsToo:\n",
    "        com = getTweetsFromAllUsersFromCommunity(timeStamp, level, community, dateMargin) \n",
    "    else:\n",
    "        com = getTweetsFromAllUsersAndUsersConnectionsFromCommunity(timeStamp, level, community, dateMargin)\n",
    "            \n",
    "    preProcessedCom = [item for sublist in com[0] for item in sublist]\n",
    "    bow = model.id2word.doc2bow(preProcessedCom) # convert to bag of words format first\n",
    "    doc_topics, word_topics, phi_values = model.get_document_topics(bow, per_word_topics=True,minimum_probability=-1.0)\n",
    "    \n",
    "    return [couple for couple in sorted(doc_topics,key= lambda a: a[1], reverse=True) if couple[1] > minWeight]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCompactTopicDistForAllComs(model, timeStamp, level, minWeight, dateMargin, connectionsToo):\n",
    "    \"\"\"\n",
    "    returns list of topic distributions. Each line corresponds to a community.\n",
    "    No fancy display.\n",
    "    \n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    :param level: is the level for the date (ex: 7)\n",
    "    :param connectionsToo: true if you want users connections tweets too, false if not\n",
    "    :param model: (gensim.models.ldamulticore): is the old model\n",
    "    :param dateMargin: is the maximal time (in days) between the date of publication of a tweet and the timeStamp (ex: 10) \n",
    "    :param minWeight: minimal weight for a topic in community\n",
    "\n",
    "\n",
    "    :type model: gensim.models.ldamulticore\n",
    "    :type timeStamp: str\n",
    "    :type level: int\n",
    "    :type connectionsToo: bool\n",
    "    :type dateMargin: int\n",
    "    :type minWeight: float\n",
    "    \n",
    "    :return: list of topic dists\n",
    "    :rtype: list\n",
    " \n",
    "    \"\"\"\n",
    "    \n",
    "    comsTopicDists = {}\n",
    "    levelComs = getCommunitiesFromLevel(timeStamp,level).keys()\n",
    "    for community in levelComs:\n",
    "        comsTopicDists[community] = getComTopicDist(community=community,\n",
    "                                              model=model,\n",
    "                                              minWeight=minWeight,\n",
    "                                              timeStamp=timeStamp,\n",
    "                                              level=level,\n",
    "                                              dateMargin=dateMargin,\n",
    "                                              connectionsToo=connectionsToo)\n",
    "    \n",
    "    return comsTopicDists\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelTopicDistribution(timeStamp, level, model, docTopicDistSorted):\n",
    "    \"\"\"\n",
    "    get for each community, the topical distribution\n",
    "        (ex: com \"x\": topic 1 80%, topic 2 20%) \n",
    "        \n",
    "    :param timeStamp: is the date (ex: 'Fri Apr 14 09:58:53 +0000 2017')\n",
    "    :param level: is the level for the date (ex: 7)\n",
    "    :param model: (gensim.models.ldamulticore): is the old model\n",
    "    :param docTopicDistSorted: community topic distributions\n",
    "\n",
    "\n",
    "    :type model: gensim.models.ldamulticore\n",
    "    :type timeStamp: str\n",
    "    :type level: int\n",
    "    :type docTopicsDistSorted: list\n",
    "\n",
    "    :return: list of strings\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    \n",
    "    communityTopics = list(zip([\", \".join([str(couple[0])+\" -> \"+str(float(\"{:.2f}\".format(couple[1]))*100)+\"%\" for couple in lst]) for lst in docTopicDistSorted], getCommunitiesFromLevel(timeStamp,level).keys()))\n",
    "    \n",
    "    return [str(\"community n° \"+str(community[1])+\" topics: \"+community[0]) for community in communityTopics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getModelTopicDistribution('Sun Apr 30 10:30:11 +0000 2017',3,myModel,myDocTopicDistSorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualization\n",
    "<div id='6'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Produce community topic distribution Word Clouds \n",
    "<div id='6-1'></div>\n",
    "For each community we have it's corresponding topic distribution. This part of the notebook allows us to display a set of word clouds for a given community. For example, if we are interested in community x (topic 0 80%, topic 5 20%), then we'll display a word cloud for topic 0 at 100% size, and a word cloud for topic 5 at 25% size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCloudsFromCommunity(model, comTopicDist):\n",
    "    \"\"\"\n",
    "    get word clouds from community\n",
    "    \n",
    "    :param model: the model\n",
    "    :param comTopicDist: the community topic distribution dict\n",
    "    \n",
    "    :type model: gensim.models.ldamulticore\n",
    "    :type comTopicDist: dict\n",
    "    \n",
    "    :return: -list of word clouds\n",
    "             -list of word clouds size dimensions (smaller for less prevalent topics)\n",
    "             \n",
    "    :rtype: (list,list)\n",
    "    \"\"\"    \n",
    "    #comTopicDistAsList = list(comTopicDist.values())[0]\n",
    "\n",
    "    wordclouds = [WordCloud(width=1000,height=1000,background_color=\"white\",min_font_size = 20).fit_words(dict(model.show_topic(t[0],200))) for t in comTopicDist]\n",
    "    figSizeDimensions = [10.0*(t[1]/comTopicDist[0][1]) for t in comTopicDist]\n",
    "\n",
    "    return wordclouds, figSizeDimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotWordCloudsFromCommunity(model, comTopicDist):\n",
    "    \"\"\"\n",
    "    plot topic model word clouds for a community\n",
    "    \n",
    "    :param model: the model\n",
    "    :param comTopicDist: the community topic distribution dict\n",
    "    \n",
    "    :type model: gensim.models.ldamulticore\n",
    "    :type comTopicDist: dict\n",
    "    \"\"\"\n",
    "    \n",
    "    wordClouds, figDimensions = getWordCloudsFromCommunity(model, comTopicDist)\n",
    "    \n",
    "    i = 0\n",
    "    for wordcloud in wordClouds:\n",
    "        plt.figure(figsize = (figDimensions[i], figDimensions[i]), facecolor = None) \n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.show()\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comTopicDist = getComTopicDist(community=b'-30000094722',timeStamp='Sun Apr 30 10:30:11 +0000 2017',level=3, model=myModel,ppComsDict=myppComs,minWeight=0.05)\n",
    "#plotWordCloudsFromCommunity(myModel,comTopicDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Visualize keyword summarization (word cloud)\n",
    "<div id='6-2'></div>\n",
    "Just like previously, we'll use word clouds to illustrate the results of a keyword summarization. Except unlike for the community topic distribution word clouds, here there will only be one word cloud containing the keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showKeyWordsWordCloud(model):\n",
    "    \"\"\"\n",
    "    show community keywords as word cloud\n",
    "    \n",
    "    :param model: the model\n",
    "    \n",
    "    :type model: gensim.models.ldamulticore\n",
    "    \"\"\"\n",
    "    \n",
    "    # plot the WordCloud image                        \n",
    "    plt.figure(figsize = (10, 10), facecolor = None) \n",
    "    plt.imshow(WordCloud( width=1000,height=1000,\n",
    "               background_color ='white', \n",
    "               min_font_size = 20).fit_words(dict(model)))\n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showKeyWordsWordCloud(myKWModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
